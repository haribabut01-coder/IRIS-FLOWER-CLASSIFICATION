# ==========================================================
# üå∏ IRIS FLOWER CLASSIFICATION PROJECT
# ==========================================================

# Step 1Ô∏è‚É£: Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# ==========================================================
# Step 2Ô∏è‚É£: Load and explore the dataset
# ==========================================================
iris = load_iris()

# Convert dataset to pandas DataFrame
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['species'] = iris.target
df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print("üîπ First 5 rows of dataset:")
print(df.head())

print("\nüîπ Dataset Information:")
print(df.info())

print("\nüîπ Statistical Summary:")
print(df.describe())

# ==========================================================
# Step 3Ô∏è‚É£: Data Visualization
# ==========================================================

# Pairplot for visualizing relationships
sns.pairplot(df, hue="species", palette="Set2")
plt.suptitle("Pair Plot of Iris Dataset", y=1.02)
plt.show()

# Heatmap for correlation
plt.figure(figsize=(6, 4))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# ==========================================================
# Step 4Ô∏è‚É£: Data Preprocessing
# ==========================================================
X = df.iloc[:, :-1]  # Independent features
y = df.iloc[:, -1]   # Target variable

# Split dataset into training and testing sets (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ==========================================================
# Step 5Ô∏è‚É£: Model Training with Multiple Algorithms
# ==========================================================
models = {
    "Logistic Regression": LogisticRegression(),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Support Vector Machine": SVC(kernel='linear', random_state=42)
}

accuracy_results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracy_results[name] = acc
    
    print(f"\nüîπ Model: {name}")
    print("Accuracy:", round(acc*100, 2), "%")
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

# ==========================================================
# Step 6Ô∏è‚É£: Model Comparison Visualization
# ==========================================================
plt.figure(figsize=(8, 4))
plt.barh(list(accuracy_results.keys()), list(accuracy_results.values()), color='skyblue')
plt.title("Model Accuracy Comparison")
plt.xlabel("Accuracy")
plt.xlim(0.8, 1.0)
plt.show()

# ==========================================================
# Step 7Ô∏è‚É£: Selecting the Best Model
# ==========================================================
best_model_name = max(accuracy_results, key=accuracy_results.get)
best_model = models[best_model_name]
print(f"\n‚úÖ Best Performing Model: {best_model_name}")
print(f"üéØ Accuracy: {accuracy_results[best_model_name]*100:.2f}%")

# ==========================================================
# Step 8Ô∏è‚É£: Predicting New Data
# ==========================================================
# Example Input: [sepal length, sepal width, petal length, petal width]
sample = np.array([[5.1, 3.5, 1.4, 0.2]])
sample_scaled = scaler.transform(sample)
prediction = best_model.predict(sample_scaled)

print(f"\nüå∏ Predicted Species for input {sample.tolist()} is ‚û°Ô∏è {prediction[0]}")

# ==========================================================
# Step 9Ô∏è‚É£: Save the Model (Optional)
# ==========================================================
import joblib
joblib.dump(best_model, "iris_model.pkl")
print("\nüíæ Model saved successfully as iris_model.pkl")
